import tensorflow as tf 
import nltk
import re

from nltk.tokenize import word_tokenize
from nltk.tokenize import WordPunctTokenizer
from tensorflow.keras.preprocessing.text import text_to_word_sequence

#필요한 파일 다운로드 
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')
nltk.download('stopwords')

#단어를 기준으로 토큰화 
#각 토크나이저의 결과가 다름
print(' 단 어 토 큰 화 1 :',word_tokenize("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."))
print(' 단 어 토 큰 화 2 :',WordPunctTokenizer().tokenize("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."))
print(' 단 어 토 큰 화 3 :',text_to_word_sequence("Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop."))

from nltk.tokenize import TreebankWordTokenizer
tokenizer = TreebankWordTokenizer()
text = "Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own."
print(' 트 리 뱅 크 워 드 토 크 나 이 저 :',tokenizer.tokenize(text))

#문장을 기준으로 토큰화 
from nltk.tokenize import sent_tokenize
text = "His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near."
print(' 문 장 토 큰 화 1 :',sent_tokenize(text))

text = "I am actively looking for Ph.D. students. and you are a Ph.D student." 
print(' 문 장 토 큰 화 2 :',sent_tokenize(text))

#단어 토큰화 후 단어에 품사 태깅 
from nltk.tag import pos_tag
text = "I am actively looking for Ph.D. students. and you are a Ph.D. student."
tokenized_sentence = word_tokenize(text)
print(' 단 어 토 큰 화 :',tokenized_sentence) 
print(' 품 사 태 깅 :',pos_tag(tokenized_sentence))

#한국어 토크나이저 
from konlpy.tag import Okt
from konlpy.tag import Kkma

okt = Okt()
kkma = Kkma()

print('OKT 형태소 분석 :',okt.morphs("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print('OKT 품사 태깅 :',okt.pos("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print('OKT 명사 추출 :',okt.nouns("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))

print('꼬꼬마 형태소 분석 :',kkma.morphs("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print('꼬꼬마 품사 태깅 :',kkma.pos("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))
print('꼬꼬마 명사 추출 :',kkma.nouns("열심히 코딩한 당신, 연휴에는 여행을 가봐요"))

'''
otk보다 kkm가 더 정밀한 형태소 분석을 할  수 있다. 
otk는 kkm에 비해 조사 형태소 분석이 더 잘 되는 느낌
명사추출 품사태깅 능력은 비슷한듯
애초에 조사를 세밀하게 형태소 분석하는게 의미가 있나.. 싶긴 함
'''

#영어에서의 불용어 필터링 
#영어의 불용어는 길이가 짧은 단어라고 생각해도 문제가 없는 경우가 많음
text = "I was wondering if anyone out there could enlighten me on this car."
shortword = re.compile(r'\W*\b\w{1,2}\b') #re모듈을 이용한 길이가 1,2인 단어 패턴 컴파일
print(shortword.sub('', text)) #sub method는 컴파일된 패턴을 만족하는 경우 원하는 문자열로 바꿈. 여기서는 공백으로

from nltk.stem import WordNetLemmatizer

#lemmatization 실습 
lemmatizer = WordNetLemmatizer()
words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']
print(' 표 제 어 추 출 전 :',words)
print(' 표 제 어 추 출 후 :',[lemmatizer.lemmatize(word) for word in words])
#표 제 어 추 출 후 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']
#dy같은 말이 안되는 단어가 포함됨. 

#구체적 품사를 알려준다면 더 정확한 lemmatization을 할 수 있음
#lemmatization : 표제어 추출
print("lemmatization of 'dies': ",lemmatizer.lemmatize('dies', 'v'))

#stemming(: 어간 추출) 실습
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

stemmer = PorterStemmer()
sentence = "This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes."

tokenized_sentence = word_tokenize(sentence)
print(' 어 간 추 출 전 :', tokenized_sentence)
print(' 어 간 추 출 후 :',[stemmer.stem(word) for word in tokenized_sentence])

#poter stemmer의 한계
#정해진 규칙을 통해 stemming하기 때문에 단어의 의미가 달라질 수 있다. 
words = ['formalize', 'allowance', 'electricical']
print(' 어 간 추 출 전 :',words)
print(' 어 간 추 출 후 :',[stemmer.stem(word) for word in words])

#lancaster stemmer vs poter stemmer 
#둘중 어떤 스테마이져를 사용할지 결과 확인 후 결정
from nltk.stem import LancasterStemmer
porter_stemmer = PorterStemmer()
lancaster_stemmer = LancasterStemmer()

words = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']
print(' 어 간 추 출 전 :', words)
print(' 포 터 스 테 머 의 어 간 추 출 후 :',[porter_stemmer.stem(w) for w in words]) 
print(' 랭 커 스 터 스 테 머 의 어 간 추 출 후 :',[lancaster_stemmer.stem(w) for w in words])

from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize 
from konlpy.tag import Okt

#불용어 : 의미파악에 필요 없는 부분
stop_words_list = stopwords.words('english') 
print(' 불 용 어 개 수 :', len(stop_words_list)) 
print(' 불 용 어 10 개 출 력 :',stop_words_list[:10])
print(type(stop_words_list))

#한국어 불용어 제거 
#보편적으로 이용되는 불용어와 사용자가 지정한 불용어를 이용해 불용어를 제거할 수 있음
example = "고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예 컨대 삼겹살을 구울 때는 중요한 게 있지."
stop_words = "를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는"

stop_words = stop_words.split(' ')
word_tokens = okt.morphs(example) 

result = [word for word in word_tokens if word not in stop_words]

print(result)

#정규표현식 실습 

r = re.compile("a.c") #.은 임의의 하나의 문자를 의미함 ex) abc, acc,akc
print(r.search('kkk')) #아무것도 매치되지 않음
print(r.search('adc')) #<re.Match object; span=(0, 3), match='adc'> 

r = re.compile('ab?c') #?는 앞 문자가 있어도 되고 없어도 된다는 의미. b는 있어도 되고 없어도 됨 -> 즉, ac와 abc매치함. 
print(r.search('abc')) #abc매치
print(r.search('ac')) #ac매치 
print(r.search('abbcc')) #None 

r = re.compile('ab*c') #b가 0개 이상 존재하는 경우 ex) ac, abc,abbbbb...c
print(r.search('a')) #none
print(r.search('ac')) #ac매치
print(r.search('abbbbbbbc')) #abbbbbbbc매치

r=re.compile('ab+c') #b가 한개 이상 존재하는 경우. ac는 매치되지 않음
print(r.search('ac')) #none
print(r.search('abc')) #abc 매치
print(r.search('abbbbbc')) #abbbbbc 매치

r = re.compile('^ab') #ab로 시작하는 문자열 
print(r.match('abaaaa')) #ab매치
print(r.match('acbbab')) #none

r = re.compile('ab{2}c') #abbc로 시작하는 문자열
print(r.match('abbc')) #abbc매치
print(r.match('abbcc'))#abbc매치
print(r.match('abkc')) #none
print(r.match('zabbc')) #none

r = re.compile('[a-d]') #a와 d사이의 알파벳중 하나와 매치함 []는 숫자, 대문자로도 적용이 가능함 
print(r.search('a')) #a match
print(r.search('eeee')) #none
print(r.search('abee')) #match a 
print(r.search('bcad')) #match b 

r = re.compile('[^a-d]') #a-d가 없는 문자열 매치
print(r.search('abc')) #none
print(r.search('eee')) #match e 
print(r.search('fge')) #match f

'''
search vs match 
search -> 문자열 전체에 대해 체크 
match -> 앞부분부터 체크 
search는 문자열 중간에 패턴이 확인되면 찾아짐
match는 문자열 앞에서 패턴과 불일치한다면 끝
'''
#search와 match의 비교 
r = re.compile('ab.') #ab뒤에 임의의 문자를 포함하는 경우
#문자열의 앞에 패턴이 포함된 경우 match와 search는 동등함
print(r.match('abzzzz')) #match abz 
print(r.search('abzzzz')) #match abz 
#문자열 처음부터 패턴이 포함되는 경우가 아니라면 match는 none 출력
print(r.match('zzzabc')) #none
print(r.search('zzzabc')) #match abc

#re.split vs str.split
#re.split이 더 복잡한 패턴에 대해, 더 다양한 기준으로 split 할 수 있음
#토큰화에 유용하게 이용할 수 있음.
'''
text = '사과 고양이 고기 으으으 졸리다'
re.split(' ', text)
'''

#re.findall 
#정규표현식과 매치되는 모든 문자열을 리스트 객체로 반환 
'''
text = '아 123 와 456 하 789'
print(re.findall("\d+",text)) 
print(re.findall("\d+",'text'))
'''
#re.sub() 
#문자열이 아닌 부분을 대체하는 예제 
'''
text = "Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern."
preprocessed_text = re.sub('[^a-zA-Z]', ' ', text) #정규표현식 ^이용
print(preprocessed_text)
'''
###텍스트 전처리 with re모듈 
text = """100 John    PROF
101 James   STUD
102 Mac   STUD"""

tokenized_text = re.split("\s+", text) #\s+ : 공백 1개 이상 찾기 
print(tokenized_text)
print(re.findall('\d+',text)) #syntax 공부 : findall(정규표현식, 문자열) -> 문자열 자리에는 str객체가 들어가야 함. 리스트 등 다른 자료형 x
print(re.findall('[A-Z]',text)) 
print(re.findall('[A-Z]{4}',text))
print(re.findall('[A-Z][a-z]+',text))

from nltk.tokenize import RegexpTokenizer
#RegexpTokenizer : 정규표현식을 이용한 단어 토큰화

text1 = "Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop"
tokenizer1 = RegexpTokenizer("[\w]+") #[\w]+ : 문자 또는 숫자가 1개 이상인 경우
#gaps = True : 정규 표현식을 토큰화의 기준으로 사용하겠다는 의미
#\s+ : 공백
tokenizer2 = RegexpTokenizer("\s+", gaps=True) 

print('[\w]+ 토큰화 결과 : ',tokenizer1.tokenize(text1)) 
print('공백 기준 토큰화 결과 : ',tokenizer2.tokenize(text1))
#두 토큰화의 결과가 다르다. 

###정수 인코딩
#단어를 정수와 mapping
raw_text = "A barber is a person. a barber is good person. a barber is huge person. he Knew A Secret! The Secret He Kept is huge secret. Huge secret. His barber kept his word. a barber kept his word. His barber kept his secret. But keeping and keeping such a huge secret to himself was driving the barber crazy. the barber went up a huge mountain."
sentences = sent_tokenize(raw_text) #문장으로 토큰화
#print(sentences)
vocab = {} #단어 : 등장 빈도
preprocessed_sentences = []

#stop_words_list : 영어 불용어 리스트
for sentence in sentences : 
    TS = word_tokenize(sentence) 
    result = [] #result list is reset when target sentence changes.  

    for word in TS :
        word = word.lower() #소문자화 
        
        if word not in stop_words_list and len(word) > 2 : #단어가 길이가 3 이상인 불용어가 아닌 경우
            result.append(word)  #append to result list 
            if word not in vocab :  #if the word doesn't exist in vocab dictionary 
                vocab[word] = 0 #append the word to dict.  
            vocab[word] += 1 #count frequency of word  
    preprocessed_sentences.append(result) #append result to preprocessed sentences list 
    #because this line is in first for loop, result list is a list whose elements are filtered words

print('preprocessed text : ',preprocessed_sentences)
print('frequency of the vocab : ',vocab)

#빈도순으로 정렬하기 
vocab_sorted = sorted(vocab.items(), key = lambda x:x[1],reverse = True) 
print('sorted by frequency',vocab_sorted)

#빈도순으로 정수 부여 
#빈도가 클수록 작은 정수 부여 
word_to_int = {} 
i = 0 
for (word,frequency) in vocab_sorted : 
    if frequency > 1 : #1번 나오는 단어는 제외 
        i += 1 
        word_to_int[word] = i 
print(word_to_int)
